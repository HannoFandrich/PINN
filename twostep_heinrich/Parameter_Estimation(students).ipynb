{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b666b863-8f8d-47eb-9488-d713a0a7d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbb14a1-a0c8-474a-a062-0548b9bab296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default style\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['legend.title_fontsize'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# deciding on color map\n",
    "cmap = cm.turbo\n",
    "\n",
    "# style for errorplot\n",
    "plot_style_dict = {'fmt' : 'o', 'capsize' : 2, 'markersize' : 3,}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11a926c0-1e12-4de5-ad9f-2ca2baaadc8b",
   "metadata": {},
   "source": [
    "# Defining Model Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e729b35f-6aab-4d8d-a52b-c0b2f6ea80c4",
   "metadata": {},
   "source": [
    "## 1. Problem Formulation\n",
    "\n",
    "Imagine you spend some days developing a model describing the following biochemical process. \n",
    "\n",
    "<img src=\"model_scheme_1.png\" width=\"250\"/>\n",
    "\n",
    "You decided to use simple mass action kinetics and derive the following ODEs:\n",
    "\n",
    "$\\frac{dY_1}{dt} = -k_1 Y_1 \\cdot k_3 Y_3$\n",
    "\n",
    "$\\frac{dY_2}{dt} = k_1 Y_1 \\cdot k_3 Y_3 - k_2 Y_2$\n",
    "\n",
    "$\\frac{dY_3}{dt} = k_2 Y_2$\n",
    "\n",
    "You realize that you don't know any of the parameter values. Unfortunately, it is impossible to measure the parameters in vitro.\n",
    "So you go into your lab and make a mix with 10 mM $Y_1$, 10 µM $Y_2$ and 10 µM $Y_3$ and measure the concentrations of all solutes over time, \n",
    "with the idea to adjust the model parameters, such that it reproduces the measurement results.\n",
    "The measured concentrations (mean +- std) are shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5969cc1-3695-4ec9-b132-8ec2f9a200df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: read the csv called two_step_data.csv as pandas daraframe and call it: two_step_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4abc19b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'two_step_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(\u001b[43mtwo_step_data\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'two_step_data' is not defined"
     ]
    }
   ],
   "source": [
    "display(two_step_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571c348-c1ff-4e50-b1fb-a1f896698a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y0_mean'], yerr=two_step_data['y0_std'], c='C0', label=r'$Y_1$',\n",
    "           **plot_style_dict)\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y1_mean'], yerr=two_step_data['y1_std'], c='C1', label=r'$Y_2$',\n",
    "           **plot_style_dict)\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y2_mean'], yerr=two_step_data['y2_std'], c='C2', label=r'$Y_3$',\n",
    "           **plot_style_dict)\n",
    "ax.set_xlabel('Time, min')\n",
    "ax.set_ylabel('Concentration, mM')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdc3e524-efdd-43df-975a-9ec5d5d0d963",
   "metadata": {},
   "source": [
    "Now you need to adjust the parameters. You start with a wild guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def5954-d00c-4ba6-bd7f-44fdb4d9502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed187909-dcf8-4892-9bef-8fbcdf444ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_step(t, y, p):\n",
    "    # TASK: write the two_step model\n",
    "\n",
    "    return [ds1, ds2, ds3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a23289-1612-4714-add0-5b3e2be789f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given are the following initial conditions and parameters\n",
    "two_step_y_init = [10, 0.01, 0.01]\n",
    "two_step_params = {'k1': 10,\n",
    "                   'k2' : 2,\n",
    "                   'k3': 5}\n",
    "\n",
    "# TASK: simulate the model from time 0 to 20. Save result points every 0.1 seconds.\n",
    "# call the result object two_step_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea5aee-3a6b-42ff-8ab4-d7a34e445b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(two_step_res.t, two_step_res.y[0])\n",
    "ax.plot(two_step_res.t, two_step_res.y[1])\n",
    "ax.plot(two_step_res.t, two_step_res.y[2])\n",
    "\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y0_mean'], yerr=two_step_data['y0_std'], c='C0', label=r'$Y_1$',\n",
    "            **plot_style_dict)\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y1_mean'], yerr=two_step_data['y1_std'], c='C1', label=r'$Y_2$',\n",
    "            **plot_style_dict)\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y2_mean'], yerr=two_step_data['y2_std'], c='C2', label=r'$Y_3$',\n",
    "            **plot_style_dict)\n",
    "ax.set_xlabel('Time, min')\n",
    "ax.set_ylabel('Concentration, mM')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69115c7a-a53f-4e48-9299-9397971574f2",
   "metadata": {},
   "source": [
    "## 2. The cost function \n",
    "\n",
    "You realize that it might be cumbersum to find the parameters manually, especially as you don't want to trust your eye to evaluate the difference between \n",
    "the model simulation results and the data.\n",
    "A friend tells you about the idea to mathematically define a score between the data and the simulation results. \n",
    "You could then try some combinations of parameters to find the parameter set with the smallest score.\n",
    "\n",
    "Deciding you want to have a minimal quadratic distance between the measured datapoints and the simulation you define the cost function (also called score or objective function):\n",
    "\n",
    "$$g(\\vec{p}) = \\sum_i (M(t_i|\\vec{p}) - y_i)^2$$\n",
    "\n",
    "where $M(t_i|\\vec{p})$ is the model prediction given parameter set $\\vec{p} = (k_1, k_2, k_3)^T$ at time $t_i$ and $y_i$ is the data value at time $t_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d67be2-b2eb-4c3d-9dd6-d2e5414ec796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_k_dict(array):\n",
    "    return {'k'+str(i+1):res for i,res in enumerate(array)}\n",
    "\n",
    "def two_step_cost_function(params_arr, model_func, data):\n",
    "    \"\"\"Objective/score/cost function for the two step model.\n",
    "    It takes parameters, the model function and data as arguments.\n",
    "    Parameters and model are use to obtain a simulation result at the time points from the data.\"\"\"\n",
    "    params = array_to_k_dict(params_arr)\n",
    "    \n",
    "    y_init = [10, 0.01, 0.01]  ## use initial values form before\n",
    "    res = solve_ivp(model_func, (0, 20), y_init, args=(params,), t_eval=data['Time'])  # simulate with parsed parameters and obtain time points corresponding to data points.\n",
    "    \n",
    "    # TASK: calculate squared distance between simulation and data for each measured species\n",
    "\n",
    "    \n",
    "    # calculate total error\n",
    "    total_error = error_1.sum() + error_2.sum() + error_3.sum()\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5ab21-57e9-4ab0-9f8b-15026c610d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cost = two_step_cost_function(list(two_step_params.values()), two_step, two_step_data)\n",
    "print(f\"Given the parameters {two_step_params['k1'] = },\\n{two_step_params['k2'] = } and\\n {two_step_params['k3'] = } result in a value of the objective function {current_cost = :0.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31fd79e6-7c24-4c8e-898a-998fe670897c",
   "metadata": {},
   "source": [
    "## 3. Minimizing the cost function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49883abc-9d78-4214-9849-f30d8234cc77",
   "metadata": {},
   "source": [
    "### Local optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8923b14c-df34-4070-998d-902680f1211c",
   "metadata": {},
   "source": [
    "### Steepest gradient descent\n",
    "\n",
    "The most elementary gradient-based method is the __gradient descent__ algorithm, in which $\\vec{p}$ is moved in the direction of the gradient $\\nabla g(\\vec{p})$ with a step size $\\alpha$:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\vec{p}_{n+1} = \\vec{p}_n - \\alpha \\nabla g(\\vec{p}_n)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The challange is to find an appropriate value for $\\alpha$. To small values result in long computation times, to large values can result in an overshoot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d999e0c3-ceea-4cab-a710-f36ba4c43ad4",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de75736e-19d0-41af-98a0-7d6169122aa0",
   "metadata": {},
   "source": [
    "One elegant way is to set the step size to the inverse of the second derivate (or Hessian ($H_g$)). Thus, when the curvature is small, the algorithm takes large steps, while in regions of high curvature the step size decreases. This is known as Newton's method. \n",
    "$$\n",
    "\\begin{equation}\n",
    " \\vec{p}_{n+1} =  \\vec{p}_n  - H_g(\\vec{p})^{-1} \\nabla g(\\vec{p})\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df9add47-9c7b-4a62-a920-07c7318981c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Levenberg Marquardt\n",
    "\n",
    "The Levenberg Marqurdt algorithm makes use of the fact, that the steepest gradient descent makes fast progress in steep section of the parameter landscape, while it struggles to converge in shallow regions. On the other hand Newton's method tends to be slow in steep regions and especially suited if close to the minimum. \n",
    "The Levenberg-Marquardt method benefits of the complementary advantages of the two methods. The Levenberg-Marquardt algorithm initiates as a gradient descent and over the course of optimization gradually switches into the Newton method.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec5bdc49-78b0-4c89-9b9d-f611a96ebd12",
   "metadata": {},
   "source": [
    "### Nelder Mead\n",
    "\n",
    "The simplex (or Nelder-Mead) algorithm initiates an adaptive, simplest polytope (simplex) with $m + 1$ vertices in $m$ dimensions (e.g. a triangle in two dimensions). The objective function is evaluated at all vertices and the worst (highest scored) vertex is reflected along the axis spanned by the others. If this reflection results in an improved score, the vertex is moved further in the reflection direction (expansion), otherwise the worst vertex is moved in direction of the remaining vertices (contraction). If none of the former operations results in an improved value of the objective function, all vertices are moved towards the best vertex. \n",
    "\n",
    "<img src=\"nelder_mead.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacd71af-ac8b-4b20-beb4-33ca0ee7909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "# import minimize function from scipy, it comes with a variety of local optimization algotihms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1cfa07c-f38b-4444-ae8d-efce412d4707",
   "metadata": {},
   "source": [
    "The [``minimize``](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function takes two required arguments. The function which return a score and an initial guess.\n",
    "It further takes additional arguments which should be passed to the cost function as the tuple ``args``. In our case we want to pass the function which defines the model and the data to the cost function.\n",
    "The optimization method can be defined as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245abfe-a37d-4243-a7bc-5c70e21178da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TASK: in general the minimize function needs only an objective (or cost) function which return a value and \n",
    "# an array-like object of parameters, which is given to this function. \n",
    "# additional arguments to the objective can be passed via the args argument.\n",
    "# FIND OUT WHAT IS MISSING BELOW:\n",
    "lhs_log_best = [0.9186961 , 0.27733539, 0.37864059]\n",
    "estimation_result = minimize(,\n",
    "                             lhs_log_best, \n",
    "                             args=(two_step, ),\n",
    "                             method='Nelder-Mead')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0ec71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6418ee60-39fc-413b-869f-adfba5ddd94d",
   "metadata": {},
   "source": [
    "The object returned by the minimization function (``estimation_result``) holds several information about the estimation process. \n",
    "Most importantly the cost and the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a644e3-79c9-4a38-9464-2399e855e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"The best parameters:\n",
    "k1 = {estimation_result.x[0]}, \n",
    "k2 = {estimation_result.x[1]},\n",
    "k3 = {estimation_result.x[2]}, \n",
    "resulting in a score of {np.round(estimation_result.fun, 2)}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d9251-2dd3-423d-bd83-114d8425b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_params_dict = array_to_k_dict(estimation_result.x)\n",
    "\n",
    "res = solve_ivp(two_step, (0, 20), two_step_y_init, args=(est_params_dict,), t_eval=np.arange(0, 20, 0.1))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(res.t, res.y[0])\n",
    "ax.plot(res.t, res.y[1])\n",
    "ax.plot(res.t, res.y[2])\n",
    "\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y0_mean'], yerr=two_step_data['y0_std'], fmt='o', capsize=2, markersize=3, c='C0')\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y1_mean'], yerr=two_step_data['y1_std'], fmt='o', capsize=2, markersize=3, c='C1')\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y2_mean'], yerr=two_step_data['y2_std'], fmt='o', capsize=2, markersize=3, c='C2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7badc19-7106-434e-bc82-7b8ce194109b",
   "metadata": {},
   "source": [
    "### Optimizing in log-space\n",
    "\n",
    "As for the sampling it might also be beneficial to let the optimization algorithm run in log space. \n",
    "There fore one can re-define the cost function, such that it takes logarithmic parameters. In this example also the cost function was altered to take into account the uncertainty in the data (i.e. the standard deviation $\\sigma$). \n",
    "\n",
    "$$g(\\vec{p}) = \\sum_i \\frac{(M(t_i|\\vec{p}) - y_i)^2}{\\sigma_i^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3e5c3-aef9-4361-b568-028be4970ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_step_cost_function_log(log_params, model_func, data):\n",
    "    \"\"\"Objective/score/cost function for the two step model.\n",
    "    It takes parameters in log-space, the model function and data as arguments.\n",
    "    Parameters and model are use to obtain a simulation result at the time points from the data.\"\"\"\n",
    "    y_init = [10, 0.01, 0.01]  ## use initial values form before\n",
    "    \n",
    "    params_arr = 10**(log_params)\n",
    "    params = array_to_k_dict(params_arr)\n",
    "    \n",
    "    res = solve_ivp(model_func, (0, 20), y_init, args=(params,), t_eval=data['Time'])  # simulate with parsed parameters and obtain time points corresponding to data points.\n",
    "    \n",
    "    # calculated squred distance between simulation and data for each measured species\n",
    "    # TASK: calculate the errors\n",
    "    \n",
    "    \n",
    "    # calculate total error\n",
    "    total_error = error_1.sum() + error_2.sum() + error_3.sum()\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d070c2-8281-42e1-9aef-63ec5d8274e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_result_log = minimize(two_step_cost_function_log, np.log10(lhs_log_best), args=(two_step, two_step_data),method='Nelder-Mead')\n",
    "best_params = 10**(estimation_result_log.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"The best parameters:\n",
    "k1 = {best_params[0]}, \n",
    "k2 = {best_params[1]},\n",
    "k3 = {best_params[2]}, \n",
    "resulting in a score of {np.round(estimation_result_log.fun, 2)}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a4288-dcda-44a6-9f17-b5a68302fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = solve_ivp(two_step,\n",
    "                (0, 20), \n",
    "                two_step_y_init, \n",
    "                args=(array_to_k_dict(best_params),),\n",
    "                t_eval=np.arange(0, 20, 0.1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(res.t, res.y[0])\n",
    "ax.plot(res.t, res.y[1])\n",
    "ax.plot(res.t, res.y[2])\n",
    "\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y0_mean'], yerr=two_step_data['y0_std'], fmt='o', capsize=2, markersize=3, c='C0')\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y1_mean'], yerr=two_step_data['y1_std'], fmt='o', capsize=2, markersize=3, c='C1')\n",
    "ax.errorbar(two_step_data['Time'], two_step_data['y2_mean'], yerr=two_step_data['y2_std'], fmt='o', capsize=2, markersize=3, c='C2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24b8318b-cfc8-4471-82b3-fe20926ae7a1",
   "metadata": {},
   "source": [
    "## 4. Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b66f7c42-0523-455a-bb30-6bd9affce0b5",
   "metadata": {},
   "source": [
    "After a while you get tired of trying new parameters and decide that the computer should do the work. \n",
    "In order to test different parameter sets you sample some datapoints within an interval you believe obtains the best parameter set $\\vec{p}^\\star$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaff7fb8-252b-422a-af18-c00866d8742f",
   "metadata": {},
   "source": [
    "### 4.1 Random\n",
    "\n",
    "Imagine you barely have an idea of the magitude of the needed parameters. \n",
    "You decide that they should be somewhere in the interval $[0.01, 10]$. \n",
    "Now you sample uniformly in this interval and hope to get a good parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206fc2f-167c-46c5-a67a-9da1c6ed6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter intervals\n",
    "lower_sample_boundary = 1e-2\n",
    "higher_sample_boundary = 10\n",
    "\n",
    "n_samples = 5 # define number of samples\n",
    "sample_dim = 3 # define sample dimension, i.e. number of parameters\n",
    "\n",
    "# create normalization function for n_samples\n",
    "norm = mpl.colors.Normalize(vmin=0,  vmax=n_samples)\n",
    "\n",
    "# draw samples within interval (note that it was assumed that the interval for each parameter is identical)\n",
    "\n",
    "# TASK: use a function from the numpy package to draw random n_samples samples which are uniformly distributed\n",
    "# within the given boundaries. Take care that each sample has the right number of parameter (sample_dim)\n",
    "\n",
    "\n",
    "print(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b6281-3c18-4f30-8bbc-49501daa8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "colors = np.arange(0, n_samples)\n",
    "\n",
    "### plot parameter bounds\n",
    "for axi in fig.axes:\n",
    "    axi.plot([lower_sample_boundary, lower_sample_boundary], [higher_sample_boundary, lower_sample_boundary], color='k')\n",
    "    axi.plot([higher_sample_boundary, lower_sample_boundary], [higher_sample_boundary, higher_sample_boundary], color='k')\n",
    "    axi.plot([higher_sample_boundary, higher_sample_boundary], [lower_sample_boundary, higher_sample_boundary], color='k')\n",
    "    axi.plot([lower_sample_boundary, higher_sample_boundary], [lower_sample_boundary, lower_sample_boundary], color='k')\n",
    "\n",
    "# plot parameter sets\n",
    "ax[0].scatter(random_samples[:,0], random_samples[:,1], c=colors,cmap=cmap)\n",
    "ax[0].set_xlabel(r'$k_1$')\n",
    "ax[0].set_ylabel(r'$k_2$')\n",
    "\n",
    "ax[1].scatter(random_samples[:,0], random_samples[:,2], c=colors,cmap=cmap)\n",
    "ax[1].set_xlabel(r'$k_1$')\n",
    "ax[1].set_ylabel(r'$k_3$')\n",
    "\n",
    "sc = ax[2].scatter(random_samples[:,1], random_samples[:,2], c=colors,cmap=cmap)\n",
    "ax[2].set_xlabel(r'$k_2$')\n",
    "ax[2].set_ylabel(r'$k_3$')  \n",
    "\n",
    "## add legend, adjust tick params etc.\n",
    "fig.legend(*sc.legend_elements(), title='Parameter Set', bbox_to_anchor=(1.2, 0.95))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10df4f85-c708-4199-ad25-d69ccc6a5122",
   "metadata": {},
   "source": [
    "The figure above shows your sampling results. Now you want to see how they compare to your data and you simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_results_for_samples(p_samples, data):\n",
    "    \n",
    "    results = []\n",
    "    scores = []\n",
    "\n",
    "    for p_vec in p_samples:\n",
    "\n",
    "        params_v = {'k1': p_vec[0], 'k2' : p_vec[1], 'k3': p_vec[2]}\n",
    "\n",
    "        two_step_res = solve_ivp(two_step,\n",
    "                                 (0, 20), \n",
    "                                 two_step_y_init, \n",
    "                                 args=(params_v,), \n",
    "                                 t_eval=np.arange(0, 20, 0.1))\n",
    "        results.append(two_step_res)\n",
    "        score = two_step_cost_function(p_vec, two_step, data)\n",
    "        scores.append(score)\n",
    "    return(scores, results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c11c09-361c-405a-ac90-88b67febb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, results = get_scores_results_for_samples(random_samples, two_step_data)\n",
    "best_index = scores.index(min(scores))\n",
    "best = random_samples[best_index]\n",
    "\n",
    "print(f\"\"\"The best parameter set is Set {best_index},\n",
    "k1 = {best[0]}, \n",
    "k2 = {best[1]},\n",
    "k3 = {best[2]}, \n",
    "resulting in a score of {min(scores)}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a9411-923c-481d-aa6d-d01c8ef5a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12,3))\n",
    "\n",
    "ax[0].errorbar(two_step_data['Time'], two_step_data['y0_mean'], yerr=two_step_data['y0_std'], c='k', label=r'$Y_1$', **plot_style_dict)\n",
    "ax[1].errorbar(two_step_data['Time'], two_step_data['y1_mean'], yerr=two_step_data['y1_std'], c='k', label=r'$Y_2$', **plot_style_dict)\n",
    "ax[2].errorbar(two_step_data['Time'], two_step_data['y2_mean'], yerr=two_step_data['y2_std'], c='k', label=r'$Y_3$', **plot_style_dict)\n",
    "\n",
    "for axi in fig.axes:\n",
    "    axi.set_xlabel('Time, min')\n",
    "    axi.set_ylabel('Concentration, mM')\n",
    "    axi.legend()\n",
    "    \n",
    "for i, res in enumerate(results):\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    color = m.to_rgba(i)\n",
    "    ax[0].plot(res.t, res.y[0], c=color)\n",
    "    ax[1].plot(res.t, res.y[1], c=color)\n",
    "    ax[2].plot(res.t, res.y[2], c=color, label=i)\n",
    "    \n",
    "lines, labels = ax[2].get_legend_handles_labels()\n",
    "\n",
    "fig.legend(lines[:-1], labels[:-1], title='Parameter Set', bbox_to_anchor=(1.05, 0.925), frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97c37c27-0387-4d49-a692-276e53508541",
   "metadata": {},
   "source": [
    "### 4.2 Latin Hypercube Sampling\n",
    "\n",
    "In some cases you realize that your uniformly chosen parameter set are actually pretty similar and you want to get a better coverage of the parameter space.\n",
    "Here, Latin Hypercube Sampling comes in handy.\n",
    "In latin hypercube sampling the range of each parameter is divided into $N$ equally probable intervals.\n",
    "LHS ensures that for each parameter, each interval contains exactly one sample (see example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8bbf8-02c3-4e3a-8724-e03e8d19e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8fad2-2056-4f68-929f-552d1cc5f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = qmc.LatinHypercube(d=sample_dim)\n",
    "lhs_sample_norm = sampler.random(n=n_samples) ## sampling is done\n",
    "\n",
    "l_bounds = [lower_sample_boundary] * sample_dim\n",
    "u_bounds = [higher_sample_boundary] * sample_dim\n",
    "\n",
    "print(f'lower bounderies: {l_bounds}, upper bounderies: {u_bounds}')\n",
    "\n",
    "lhs_sample = qmc.scale(lhs_sample_norm, l_bounds, u_bounds) ## stretch samples to bounderies\n",
    "\n",
    "print(f'Latin hypercube samples: \\n {lhs_sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620732a4-531d-4917-968d-30630d68f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(9, 6))\n",
    "\n",
    "colors = np.arange(0, n_samples)\n",
    "\n",
    "### plot parameter bounds\n",
    "for axi in fig.axes:\n",
    "    axi.plot([lower_sample_boundary, lower_sample_boundary], [higher_sample_boundary, lower_sample_boundary], color='k')\n",
    "    axi.plot([higher_sample_boundary, lower_sample_boundary], [higher_sample_boundary, higher_sample_boundary], color='k')\n",
    "    axi.plot([higher_sample_boundary, higher_sample_boundary], [lower_sample_boundary, higher_sample_boundary], color='k')\n",
    "    axi.plot([lower_sample_boundary, higher_sample_boundary], [lower_sample_boundary, lower_sample_boundary], color='k')\n",
    "\n",
    "### plot LHS grid\n",
    "lhs_vals = np.linspace(lower_sample_boundary, higher_sample_boundary, n_samples+1)\n",
    "for axi in fig.axes:\n",
    "    for lhs_val in lhs_vals:\n",
    "        axi.plot([lhs_val, lhs_val], [lower_sample_boundary, higher_sample_boundary], c='grey', alpha=0.3)\n",
    "        axi.plot([lower_sample_boundary, higher_sample_boundary], [lhs_val, lhs_val], c='grey', alpha=0.3)\n",
    "\n",
    "# plot random samples\n",
    "    \n",
    "ax[1][0].scatter(random_samples[:,0], random_samples[:,1], c='k', s=10)\n",
    "ax[0][0].scatter(lhs_sample[:,0], lhs_sample[:,1], c=colors,cmap=cmap)\n",
    "\n",
    "\n",
    "ax[1][1].scatter(random_samples[:,0], random_samples[:,2], c='k', s=10)\n",
    "ax[0][1].scatter(lhs_sample[:,0], lhs_sample[:,2], c=colors,cmap=cmap)\n",
    "\n",
    "\n",
    "ax[1][2].scatter(random_samples[:,1], random_samples[:,2], c='k', s=10, label='Uniform samples')\n",
    "sc = ax[0][2].scatter(lhs_sample[:,1], lhs_sample[:,2], c=colors,cmap=cmap)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i][0].set_xlabel(r'$k_1$')\n",
    "    ax[i][0].set_ylabel(r'$k_2$')\n",
    "\n",
    "\n",
    "    ax[i][1].set_xlabel(r'$k_1$')\n",
    "    ax[i][1].set_ylabel(r'$k_3$')\n",
    "\n",
    "    ax[i][2].set_xlabel(r'$k_2$')\n",
    "    ax[i][2].set_ylabel(r'$k_3$')\n",
    "    \n",
    "\n",
    "\n",
    "## add legend, adjust tick params etc.\n",
    "fig.legend(*sc.legend_elements(), title='Parameter Set', bbox_to_anchor=(1.2, 0.95))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68289dd4-a693-4271-b123-af395c7c207a",
   "metadata": {},
   "source": [
    "In most cases one should see that the LHS samples result in a better coverage of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d269e1-0919-406c-ad24-c76fdb520ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_scores, lhs_results = get_scores_results_for_samples(lhs_sample, two_step_data)\n",
    "\n",
    "best_index = lhs_scores.index(min(lhs_scores))\n",
    "best = lhs_sample[best_index]\n",
    "\n",
    "print(f\"\"\"The best parameter set is Set {best_index},\n",
    "k1 = {best[0]}, \n",
    "k2 = {best[1]},\n",
    "k3 = {best[2]}, \n",
    "resulting in a score of {min(lhs_scores)}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ae794-1816-4457-99b5-298a3d6cfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12,3))\n",
    "\n",
    "ax[0].errorbar(two_step_data['Time'], two_step_data['y0_mean'], yerr=two_step_data['y0_std'], fmt='o', capsize=2, markersize=3, c='k', label=r'$Y_1$')\n",
    "ax[1].errorbar(two_step_data['Time'], two_step_data['y1_mean'], yerr=two_step_data['y1_std'], fmt='o', capsize=2, markersize=3, c='k', label=r'$Y_2$')\n",
    "ax[2].errorbar(two_step_data['Time'], two_step_data['y2_mean'], yerr=two_step_data['y2_std'], fmt='o', capsize=2, markersize=3, c='k', label=r'$Y_3$')\n",
    "for axi in fig.axes:\n",
    "    axi.set_xlabel('Time, min')\n",
    "    axi.set_ylabel('Concentration, mM')\n",
    "    axi.legend()\n",
    "    \n",
    "for i, res in enumerate(lhs_results):\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    color = m.to_rgba(i)\n",
    "    ax[0].plot(res.t, res.y[0], c=color)\n",
    "    ax[1].plot(res.t, res.y[1], c=color)\n",
    "    ax[2].plot(res.t, res.y[2], c=color, label=i)\n",
    "    \n",
    "lines, labels = ax[2].get_legend_handles_labels()\n",
    "fig.legend(lines[:-1], labels[:-1], title='Parameter Set', bbox_to_anchor=(1.05, 0.925))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c88ac1eb-a334-4675-90fa-7b247b3372de",
   "metadata": {},
   "source": [
    "### 4.3 Logarithmic scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b3e633d-00b2-4eac-8b24-7efa9e60d6b7",
   "metadata": {},
   "source": [
    "In many cases you don't even now the magnitude of a parameter (i.e. if it is close to  $0.1$, $10$ or $1000$). In those cases it is beneficial to sample parameters logarithmically.\n",
    "Furthermore, the effect of parameter changes on model results are often non-linear. A change of $k_1$ from $9$ to $10$ probably has a smaller effect than a change from $0.1$ to $1.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271009c-183e-4a46-89ef-aa3e196986a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = qmc.LatinHypercube(d=sample_dim)\n",
    "\n",
    "# get lhs samples\n",
    "lhs_log_sample = sampler.random(n=n_samples)\n",
    "\n",
    "# transform boundaries to log scale\n",
    "l_bounds = [np.log10(lower_sample_boundary)]*sample_dim\n",
    "u_bounds = [np.log10(higher_sample_boundary)]*sample_dim\n",
    "\n",
    "# scale samples according to logarithmic boundaries\n",
    "lhs_log_sample = qmc.scale(lhs_log_sample, l_bounds, u_bounds)\n",
    "\n",
    "# get parameter values from log LHS samples\n",
    "lhs_log_sample = 10**(lhs_log_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4ca93-2389-4344-adc2-3c560ee37dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_log_scores, lhs_log_results = get_scores_results_for_samples(lhs_log_sample, two_step_data)\n",
    "\n",
    "best_index = lhs_log_scores.index(min(lhs_log_scores))\n",
    "lhs_log_best = lhs_log_sample[best_index]\n",
    "\n",
    "print(f\"\"\"The best parameter set is Set {best_index},\n",
    "k1 = {best[0]}, \n",
    "k2 = {best[1]},\n",
    "k3 = {best[2]}, \n",
    "resulting in a score of {min(lhs_log_scores)}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa38570-4609-40b6-a2b1-988e39376b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12,3))\n",
    "\n",
    "ax[0].errorbar(two_step_data['Time'], two_step_data['y0_mean'], yerr=two_step_data['y0_std'], fmt='o', capsize=2, markersize=3, c='k', label=r'$Y_1$')\n",
    "ax[1].errorbar(two_step_data['Time'], two_step_data['y1_mean'], yerr=two_step_data['y1_std'], fmt='o', capsize=2, markersize=3, c='k', label=r'$Y_2$')\n",
    "ax[2].errorbar(two_step_data['Time'], two_step_data['y2_mean'], yerr=two_step_data['y2_std'], fmt='o', capsize=2, markersize=3, c='k', label=r'$Y_3$')\n",
    "for axi in fig.axes:\n",
    "    axi.set_xlabel('Time, min')\n",
    "    axi.set_ylabel('Concentration, mM')\n",
    "    axi.legend()\n",
    "    \n",
    "for i, res in enumerate(lhs_log_results):\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    color = m.to_rgba(i)\n",
    "    ax[0].plot(res.t, res.y[0], c=color)\n",
    "    ax[1].plot(res.t, res.y[1], c=color)\n",
    "    ax[2].plot(res.t, res.y[2], c=color, label=i)\n",
    "    \n",
    "lines, labels = ax[2].get_legend_handles_labels()\n",
    "fig.legend(lines[:-1], labels[:-1], title='Parameter Set', bbox_to_anchor=(1.05, 0.925))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "689bd7b6-d072-419d-ae06-f4ba9906fb27",
   "metadata": {},
   "source": [
    "Although the sampling helped you to get a better approximation of the parameter set, you are not at all satisfied.\n",
    "Thus you look for local optimization algorithms which will help you to find the best fit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2a2a108-214b-4686-b2cb-1c6e6dfa7e92",
   "metadata": {},
   "source": [
    "## 5. Your turn\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65291289-7cdf-407b-b291-76ac72259a9f",
   "metadata": {},
   "source": [
    "Now its your turn. You are given the dataset ``heinrich.csv`` and the ODE system:\n",
    "\n",
    "$$\n",
    "\\frac{dy_0}{dt} = p_0 - y_0 - (p_1\\cdot y_0)\\cdot(1 + p_2\\cdot y_1^4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy_1}{dt} = (p_1\\cdot y_0)\\cdot(1 + p_2\\cdot y_1^4) - p_3\\cdot y_1\n",
    "$$\n",
    "\n",
    "with the initial values $y_0 = 1$ and $y_1 = 0$. \n",
    "Find parameters which fit the data well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aed30f0-b36c-40cc-86e1-0baf7894166e",
   "metadata": {},
   "source": [
    "## 5.1 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7139b-9802-417c-a9cf-173394fa6ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc12baf8-8cb1-4a97-9f72-54e343755459",
   "metadata": {},
   "source": [
    "## 5.2 Define the model as python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77148c6-b7bd-42df-b30d-cc07c70e5bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ad0c953-4892-4f4c-a030-9690d2993d34",
   "metadata": {},
   "source": [
    "## 5.3 Define the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d336d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e8a17-eeac-4424-b344-0c5b0296e0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f7292b2-9427-4ae0-9076-57110eebcd38",
   "metadata": {},
   "source": [
    "## 5.4 Sample logarithmically \n",
    "\n",
    "To find a good initial parameter set for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eda141-3827-4ae0-a640-a7992d43dafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55425eba-2df4-40ec-ba3a-90254c3bade0",
   "metadata": {},
   "source": [
    "## 5.5 Run the optimization \n",
    "\n",
    "WARNING: It might be neccessary to use every sampling result as input for the optimization, as the given model is not too easy to fit.\n",
    "In this case you want to iterate over the samples and store the estimation results in a list to later check which one performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cd1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5499375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832bd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2922b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95867dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc03912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f57082ed-cc2d-4250-91b5-1ecd5575daef",
   "metadata": {},
   "source": [
    "# Appendix: Creation of Pseudo Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "606c16eb-01fd-4904-bc5b-89ac625e7fb3",
   "metadata": {},
   "source": [
    "## Heinrich Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4aa2fd-170b-457c-a43f-e1c5e575995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_init = [1, 0]\n",
    "params = [6.77, 1.01, 1.26, 5.11]\n",
    "res = solve_ivp(heinrich_model, (0, 10), y_init, args=(params,), t_eval=np.arange(0, 10, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e29678-3a0d-4fe0-ae40-8ae2c8d24e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 6\n",
    "th_point = 5\n",
    "\n",
    "pseudo_time = res.t[::th_point]\n",
    "psd_1 = []\n",
    "psd_2 = []\n",
    "for i in range(reps):\n",
    "    pseudo_data_y1 = res.y[0,::th_point] + np.random.normal(0, res.y[0,::th_point]/10)\n",
    "    pseudo_data_y2 = res.y[1,::th_point]+ np.random.normal(0, res.y[1,::th_point]/5)\n",
    "    psd_1.append(pseudo_data_y1)\n",
    "    psd_2.append(pseudo_data_y2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef767fc-57e9-465f-83f0-4851cdc7e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_1_df = pd.DataFrame(psd_1).T\n",
    "psd_2_df = pd.DataFrame(psd_2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454a886-967c-428a-982c-f31ee0e92544",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_cols = [f'Rep{i}' for i in range(reps)]\n",
    "\n",
    "psd_1_df.columns = rep_cols\n",
    "psd_1_df['mean'] = psd_1_df[rep_cols].mean(axis=1)\n",
    "psd_1_df['std'] = psd_1_df[rep_cols].std(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ff642-fa27-4978-ac6d-7f3e1e5b3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_cols = [f'Rep{i}' for i in range(reps)]\n",
    "\n",
    "psd_2_df.columns = rep_cols\n",
    "psd_2_df['mean'] = psd_2_df[rep_cols].mean(axis=1)\n",
    "psd_2_df['std'] = psd_2_df[rep_cols].std(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15f528-d68a-4356-ad62-da682c572428",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(columns=['Time', 'y0_mean', 'y0_std', 'y1_mean', 'y1_std'])\n",
    "dataset['Time'] = pseudo_time\n",
    "dataset['y0_mean'] = psd_1_df['mean']\n",
    "dataset['y0_std'] = psd_1_df['std']\n",
    "dataset['y1_mean'] = psd_2_df['mean']\n",
    "dataset['y1_std'] = psd_2_df['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d56a3-f6ce-48c9-af77-e0264f35579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('heinrich.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f896cdf-240d-4054-849c-0187ffb4651a",
   "metadata": {},
   "source": [
    "## Two Step Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43913c2-0799-4291-99a9-2b5ec866c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_init = [10, 0.01, 0.01]\n",
    "params = [0.3, 0.2, 2]\n",
    "res = solve_ivp(two_step, (0, 20), y_init, args=(params,), t_eval=np.arange(0, 20, 0.1))\n",
    "\n",
    "reps = 6\n",
    "th_point = 10\n",
    "\n",
    "pseudo_time = res.t[::th_point]\n",
    "psd_1 = []\n",
    "psd_2 = []\n",
    "psd_3 = []\n",
    "for i in range(reps):\n",
    "    pseudo_data_y1 = res.y[0,::th_point] + np.random.normal(0,0.2+ res.y[0,::th_point]/20+ res.t[::th_point]/200)\n",
    "    pseudo_data_y2 = res.y[1,::th_point]+ np.random.normal(0,0.4+ res.y[1,::th_point]/20+ res.t[::th_point]/200)\n",
    "    pseudo_data_y3 = res.y[2,::th_point]+ np.random.normal(0,0.4+ res.y[2,::th_point]/20+ res.t[::th_point]/200)\n",
    "    \n",
    "    psd_1.append(pseudo_data_y1)\n",
    "    psd_2.append(pseudo_data_y2) \n",
    "    psd_3.append(pseudo_data_y3)    \n",
    "    \n",
    "    \n",
    "psd_1_df = pd.DataFrame(psd_1).T\n",
    "psd_2_df = pd.DataFrame(psd_2).T\n",
    "psd_3_df = pd.DataFrame(psd_3).T\n",
    "\n",
    "\n",
    "rep_cols = [f'Rep{i}' for i in range(reps)]\n",
    "\n",
    "psd_1_df.columns = rep_cols\n",
    "psd_1_df['mean'] = psd_1_df[rep_cols].mean(axis=1)\n",
    "psd_1_df['std'] = psd_1_df[rep_cols].std(axis=1)\n",
    "\n",
    "psd_2_df.columns = rep_cols\n",
    "psd_2_df['mean'] = psd_2_df[rep_cols].mean(axis=1)\n",
    "psd_2_df['std'] = psd_2_df[rep_cols].std(axis=1)\n",
    "\n",
    "psd_3_df.columns = rep_cols\n",
    "psd_3_df['mean'] = psd_3_df[rep_cols].mean(axis=1)\n",
    "psd_3_df['std'] = psd_3_df[rep_cols].std(axis=1)\n",
    "\n",
    "dataset = pd.DataFrame(columns=['Time', 'y0_mean', 'y0_std', 'y1_mean', 'y1_std', 'y2_mean', 'y2_std'])\n",
    "dataset['Time'] = pseudo_time\n",
    "dataset['y0_mean'] = psd_1_df['mean']\n",
    "dataset['y0_std'] = psd_1_df['std']\n",
    "dataset['y1_mean'] = psd_2_df['mean']\n",
    "dataset['y1_std'] = psd_2_df['std']\n",
    "dataset['y2_mean'] = psd_3_df['mean']\n",
    "dataset['y2_std'] = psd_3_df['std']\n",
    "\n",
    "dataset.to_csv('two_step_dataset.csv', index=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(res.t, res.y[0])\n",
    "ax.plot(res.t, res.y[1])\n",
    "ax.plot(res.t, res.y[2])\n",
    "\n",
    "\n",
    "ax.errorbar(pseudo_time, psd_1_df['mean'], yerr=psd_1_df['std'], fmt='o', capsize=2, markersize=3, c='C0')\n",
    "ax.errorbar(pseudo_time, psd_2_df['mean'], yerr=psd_2_df['std'], fmt='o', capsize=2, markersize=3, c='C1')\n",
    "ax.errorbar(pseudo_time, psd_3_df['mean'], yerr=psd_3_df['std'], fmt='o', capsize=2, markersize=3, c='C2')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a513b6-c00f-47b1-97c6-327f62773eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
